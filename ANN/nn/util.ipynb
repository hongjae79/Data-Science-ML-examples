{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some utility functions we need for the class.\n",
    "# For the class Data Science: Practical Deep Learning Concepts in Theano and TensorFlow\n",
    "# https://deeplearningcourses.com/c/data-science-deep-learning-in-theano-tensorflow\n",
    "# https://www.udemy.com/data-science-deep-learning-in-theano-tensorflow\n",
    "\n",
    "# Note: run this from the current folder it is in.\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "def get_clouds():\n",
    "    Nclass = 500\n",
    "    D = 2\n",
    "\n",
    "    X1 = np.random.randn(Nclass, D) + np.array([0, -2])\n",
    "    X2 = np.random.randn(Nclass, D) + np.array([2, 2])\n",
    "    X3 = np.random.randn(Nclass, D) + np.array([-2, 2])\n",
    "    X = np.vstack([X1, X2, X3])\n",
    "\n",
    "    Y = np.array([0]*Nclass + [1]*Nclass + [2]*Nclass)\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "def get_spiral():\n",
    "    # Idea: radius -> low...high\n",
    "    #           (don't start at 0, otherwise points will be \"mushed\" at origin)\n",
    "    #       angle = low...high proportional to radius\n",
    "    #               [0, 2pi/6, 4pi/6, ..., 10pi/6] --> [pi/2, pi/3 + pi/2, ..., ]\n",
    "    # x = rcos(theta), y = rsin(theta) as usual\n",
    "\n",
    "    radius = np.linspace(1, 10, 100)\n",
    "    thetas = np.empty((6, 100))\n",
    "    for i in range(6):\n",
    "        start_angle = np.pi*i / 3.0\n",
    "        end_angle = start_angle + np.pi / 2\n",
    "        points = np.linspace(start_angle, end_angle, 100)\n",
    "        thetas[i] = points\n",
    "\n",
    "    # convert into cartesian coordinates\n",
    "    x1 = np.empty((6, 100))\n",
    "    x2 = np.empty((6, 100))\n",
    "    for i in range(6):\n",
    "        x1[i] = radius * np.cos(thetas[i])\n",
    "        x2[i] = radius * np.sin(thetas[i])\n",
    "\n",
    "    # inputs\n",
    "    X = np.empty((600, 2))\n",
    "    X[:,0] = x1.flatten()\n",
    "    X[:,1] = x2.flatten()\n",
    "\n",
    "    # add noise\n",
    "    X += np.random.randn(600, 2)*0.5\n",
    "\n",
    "    # targets\n",
    "    Y = np.array([0]*100 + [1]*100 + [0]*100 + [1]*100 + [0]*100 + [1]*100)\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "\n",
    "def get_transformed_data():\n",
    "    print(\"Reading in and transforming data...\")\n",
    "\n",
    "    if not os.path.exists('../large_files/train.csv'):\n",
    "        print('Looking for ../large_files/train.csv')\n",
    "        print('You have not downloaded the data and/or not placed the files in the correct location.')\n",
    "        print('Please get the data from: https://www.kaggle.com/c/digit-recognizer')\n",
    "        print('Place train.csv in the folder large_files adjacent to the class folder')\n",
    "        exit()\n",
    "\n",
    "    df = pd.read_csv('../large_files/train.csv')\n",
    "    data = df.values.astype(np.float32)\n",
    "    np.random.shuffle(data)\n",
    "\n",
    "    X = data[:, 1:]\n",
    "    Y = data[:, 0].astype(np.int32)\n",
    "\n",
    "    Xtrain = X[:-1000]\n",
    "    Ytrain = Y[:-1000]\n",
    "    Xtest  = X[-1000:]\n",
    "    Ytest  = Y[-1000:]\n",
    "\n",
    "    # center the data\n",
    "    mu = Xtrain.mean(axis=0)\n",
    "    Xtrain = Xtrain - mu\n",
    "    Xtest  = Xtest - mu\n",
    "\n",
    "    # transform the data\n",
    "    pca = PCA()\n",
    "    Ztrain = pca.fit_transform(Xtrain)\n",
    "    Ztest  = pca.transform(Xtest)\n",
    "\n",
    "    plot_cumulative_variance(pca)\n",
    "\n",
    "    # take first 300 cols of Z\n",
    "    Ztrain = Ztrain[:, :300]\n",
    "    Ztest = Ztest[:, :300]\n",
    "\n",
    "    # normalize Z\n",
    "    mu = Ztrain.mean(axis=0)\n",
    "    std = Ztrain.std(axis=0)\n",
    "    Ztrain = (Ztrain - mu) / std\n",
    "    Ztest = (Ztest - mu) / std\n",
    "\n",
    "    return Ztrain, Ztest, Ytrain, Ytest\n",
    "\n",
    "\n",
    "def get_normalized_data():\n",
    "    print(\"Reading in and transforming data...\")\n",
    "\n",
    "    if not os.path.exists('../large_files/train.csv'):\n",
    "        print('Looking for ../large_files/train.csv')\n",
    "        print('You have not downloaded the data and/or not placed the files in the correct location.')\n",
    "        print('Please get the data from: https://www.kaggle.com/c/digit-recognizer')\n",
    "        print('Place train.csv in the folder large_files adjacent to the class folder')\n",
    "        exit()\n",
    "\n",
    "    df = pd.read_csv('../Data Science/train.csv')\n",
    "    data = df.values.astype(np.float32)\n",
    "    np.random.shuffle(data)\n",
    "    X = data[:, 1:]\n",
    "    Y = data[:, 0]\n",
    "\n",
    "    Xtrain = X[:-1000]\n",
    "    Ytrain = Y[:-1000]\n",
    "    Xtest  = X[-1000:]\n",
    "    Ytest  = Y[-1000:]\n",
    "\n",
    "    # normalize the data\n",
    "    mu = Xtrain.mean(axis=0)\n",
    "    std = Xtrain.std(axis=0)\n",
    "    np.place(std, std == 0, 1)\n",
    "    Xtrain = (Xtrain - mu) / std\n",
    "    Xtest = (Xtest - mu) / std\n",
    "    \n",
    "    return Xtrain, Xtest, Ytrain, Ytest\n",
    "\n",
    "\n",
    "def plot_cumulative_variance(pca):\n",
    "    P = []\n",
    "    for p in pca.explained_variance_ratio_:\n",
    "        if len(P) == 0:\n",
    "            P.append(p)\n",
    "        else:\n",
    "            P.append(p + P[-1])\n",
    "    plt.plot(P)\n",
    "    plt.show()\n",
    "    return P\n",
    "\n",
    "\n",
    "def forward(X, W, b):\n",
    "    # softmax\n",
    "    a = X.dot(W) + b\n",
    "    expa = np.exp(a)\n",
    "    y = expa / expa.sum(axis=1, keepdims=True)\n",
    "    return y\n",
    "\n",
    "\n",
    "def predict(p_y):\n",
    "    return np.argmax(p_y, axis=1)\n",
    "\n",
    "\n",
    "def error_rate(p_y, t):\n",
    "    prediction = predict(p_y)\n",
    "    return np.mean(prediction != t)\n",
    "\n",
    "\n",
    "def cost(p_y, t):\n",
    "    tot = t * np.log(p_y)\n",
    "    return -tot.sum()\n",
    "\n",
    "\n",
    "def gradW(t, y, X):\n",
    "    return X.T.dot(t - y)\n",
    "\n",
    "\n",
    "def gradb(t, y):\n",
    "    return (t - y).sum(axis=0)\n",
    "\n",
    "\n",
    "def y2indicator(y):\n",
    "    N = len(y)\n",
    "    y = y.astype(np.int32)\n",
    "    ind = np.zeros((N, 10))\n",
    "    for i in range(N):\n",
    "        ind[i, y[i]] = 1\n",
    "    return ind\n",
    "\n",
    "\n",
    "def benchmark_full():\n",
    "    Xtrain, Xtest, Ytrain, Ytest = get_normalized_data()\n",
    "\n",
    "    print(\"Performing logistic regression...\")\n",
    "    # lr = LogisticRegression(solver='lbfgs')\n",
    "\n",
    "\n",
    "    # convert Ytrain and Ytest to (N x K) matrices of indicator variables\n",
    "    N, D = Xtrain.shape\n",
    "    Ytrain_ind = y2indicator(Ytrain)\n",
    "    Ytest_ind = y2indicator(Ytest)\n",
    "\n",
    "    W = np.random.randn(D, 10) / np.sqrt(D)\n",
    "    b = np.zeros(10)\n",
    "    LL = []\n",
    "    LLtest = []\n",
    "    CRtest = []\n",
    "\n",
    "    # reg = 1\n",
    "    # learning rate 0.0001 is too high, 0.00005 is also too high\n",
    "    # 0.00003 / 2000 iterations => 0.363 error, -7630 cost\n",
    "    # 0.00004 / 1000 iterations => 0.295 error, -7902 cost\n",
    "    # 0.00004 / 2000 iterations => 0.321 error, -7528 cost\n",
    "\n",
    "    # reg = 0.1, still around 0.31 error\n",
    "    # reg = 0.01, still around 0.31 error\n",
    "    lr = 0.00004\n",
    "    reg = 0.01\n",
    "    for i in range(500):\n",
    "        p_y = forward(Xtrain, W, b)\n",
    "        # print \"p_y:\", p_y\n",
    "        ll = cost(p_y, Ytrain_ind)\n",
    "        LL.append(ll)\n",
    "\n",
    "        p_y_test = forward(Xtest, W, b)\n",
    "        lltest = cost(p_y_test, Ytest_ind)\n",
    "        LLtest.append(lltest)\n",
    "        \n",
    "        err = error_rate(p_y_test, Ytest)\n",
    "        CRtest.append(err)\n",
    "\n",
    "        W += lr*(gradW(Ytrain_ind, p_y, Xtrain) - reg*W)\n",
    "        b += lr*(gradb(Ytrain_ind, p_y) - reg*b)\n",
    "        if i % 10 == 0:\n",
    "            print(\"Cost at iteration %d: %.6f\" % (i, ll))\n",
    "            print(\"Error rate:\", err)\n",
    "\n",
    "    p_y = forward(Xtest, W, b)\n",
    "    print(\"Final error rate:\", error_rate(p_y, Ytest))\n",
    "    iters = range(len(LL))\n",
    "    plt.plot(iters, LL, iters, LLtest)\n",
    "    plt.show()\n",
    "    plt.plot(CRtest)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def benchmark_pca():\n",
    "    Xtrain, Xtest, Ytrain, Ytest = get_transformed_data()\n",
    "    print(\"Performing logistic regression...\")\n",
    "\n",
    "    N, D = Xtrain.shape\n",
    "    Ytrain_ind = np.zeros((N, 10))\n",
    "    for i in range(N):\n",
    "        Ytrain_ind[i, Ytrain[i]] = 1\n",
    "\n",
    "    Ntest = len(Ytest)\n",
    "    Ytest_ind = np.zeros((Ntest, 10))\n",
    "    for i in range(Ntest):\n",
    "        Ytest_ind[i, Ytest[i]] = 1\n",
    "\n",
    "    W = np.random.randn(D, 10) / np.sqrt(D)\n",
    "    b = np.zeros(10)\n",
    "    LL = []\n",
    "    LLtest = []\n",
    "    CRtest = []\n",
    "\n",
    "    # D = 300 -> error = 0.07\n",
    "    lr = 0.0001\n",
    "    reg = 0.01\n",
    "    for i in range(200):\n",
    "        p_y = forward(Xtrain, W, b)\n",
    "        # print \"p_y:\", p_y\n",
    "        ll = cost(p_y, Ytrain_ind)\n",
    "        LL.append(ll)\n",
    "\n",
    "        p_y_test = forward(Xtest, W, b)\n",
    "        lltest = cost(p_y_test, Ytest_ind)\n",
    "        LLtest.append(lltest)\n",
    "\n",
    "        err = error_rate(p_y_test, Ytest)\n",
    "        CRtest.append(err)\n",
    "\n",
    "        W += lr*(gradW(Ytrain_ind, p_y, Xtrain) - reg*W)\n",
    "        b += lr*(gradb(Ytrain_ind, p_y) - reg*b)\n",
    "        if i % 10 == 0:\n",
    "            print(\"Cost at iteration %d: %.6f\" % (i, ll))\n",
    "            print(\"Error rate:\", err)\n",
    "\n",
    "    p_y = forward(Xtest, W, b)\n",
    "    print(\"Final error rate:\", error_rate(p_y, Ytest))\n",
    "    iters = range(len(LL))\n",
    "    plt.plot(iters, LL, iters, LLtest)\n",
    "    plt.show()\n",
    "    plt.plot(CRtest)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "#if __name__ == '__main__':\n",
    "    # benchmark_pca()\n",
    "    #benchmark_full()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
